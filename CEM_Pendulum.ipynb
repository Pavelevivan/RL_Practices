{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CEM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJK13py21jeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import gym\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sb\n",
        "import random\n",
        "from copy import deepcopy\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkrcrgNA2smh",
        "colab_type": "text"
      },
      "source": [
        "# Solving Pendulum using CEM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRXWXH_eqO1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_session(agent, env, t_max=200):\n",
        "  \"\"\"\n",
        "    Generate session on environment with agent\n",
        "  \"\"\"\n",
        "  state = env.reset()\n",
        "  actions = []\n",
        "  states = []\n",
        "  agent.random_process.reset_states()\n",
        "  total_reward = 0\n",
        "  for _ in range(t_max):\n",
        "    action = agent.get_action(state)\n",
        "    new_s, r, done, info = env.step(action)\n",
        "    states.append(state)\n",
        "    actions.append(action)\n",
        "    total_reward += r\n",
        "    state = new_s\n",
        "    if done:\n",
        "      break\n",
        "  return states, actions, total_reward\n",
        "\n",
        "def select_elites(states, actions, rewards, percentile=50):\n",
        "  \"\"\"\n",
        "    Select sessions with the most reward\n",
        "    by percentile\n",
        "  \"\"\"\n",
        "  reward_threshold = np.percentile(rewards, percentile)\n",
        "  elite_states, elite_actions = [], []\n",
        "  for i in range(len(rewards)):\n",
        "    if rewards[i] > reward_threshold:\n",
        "      elite_states.append(states[i])\n",
        "      elite_actions.append(actions[i])\n",
        "  return elite_states, elite_actions\n",
        "\n",
        "def show_progress():\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2w4VSjH_HK5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Network(torch.nn.Module):\n",
        "  def __init__(self, state_shape):\n",
        "    super().__init__()\n",
        "    self.linear_1 = torch.nn.Linear(state_shape[0], 400)\n",
        "    self.linear_2 = torch.nn.Linear(400, 300)\n",
        "    self.linear_3 = torch.nn.Linear(300, 1)\n",
        "    # self.linear_4 = torch.nn.Linear(50, 1)\n",
        "    self.relu = torch.nn.ReLU()\n",
        "    self.tang = torch.nn.Tanh()\n",
        "\n",
        "\n",
        "  def forward(self, state):\n",
        "    predicted = self.relu(self.linear_1(state))\n",
        "    predicted = self.relu(self.linear_2(predicted))\n",
        "    predicted = self.tang(self.linear_3(predicted))\n",
        "    # predicted = self.tang(self.linear_4(predicted))\n",
        "    return predicted * 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HE9P60Jk14Wl",
        "colab_type": "code",
        "outputId": "095b422b-ccc2-49f1-c51f-a857f76516ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "class CEM_agent(torch.nn.Module):\n",
        "  def __init__(self, state_shape, action_shape, epsilon=0.2, gamma=0.99,\n",
        "               tau=1e-3, batch_size=128, learning_rate=1e-2, n_batches=16):\n",
        "    super().__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.epsilon = epsilon\n",
        "    self.gamma = gamma\n",
        "    self.noise_threshold = 1\n",
        "    self.tau = tau\n",
        "    self.noise_threshold_decrease = 1.0 / 500\n",
        "    self.noise_threshold_min = 0.001\n",
        "    self.batch_size = batch_size\n",
        "    self.n_batches = 16\n",
        "    self.loss = torch.nn.MSELoss()\n",
        "    self.network = Network(state_shape)\n",
        "    self.optimizer = torch.optim.SGD(self.network.parameters(), lr=learning_rate)\n",
        "    # self.optimizer = torch.optim.Adam(params=self.network.parameters(), lr=learning_rate)\n",
        "    self.random_process = OrnsteinUhlenbeckProcess(size=action_shape[0], theta=0.15, mu=0, sigma=0.2)\n",
        "\n",
        "  def get_batch(self, elite_states, elite_actions):\n",
        "    # несколько батчей\n",
        "    batch = random.sample(list(zip(elite_states, elite_actions)),\n",
        "                          min(len(elite_actions), self.batch_size))\n",
        "    states, actions = map(np.array, zip(*batch))\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    actions = torch.tensor(actions, dtype=torch.float32)\n",
        "    return states, actions\n",
        "  \n",
        "  def update_weights(self, old_model):\n",
        "    for new_parameter, old_parameter in zip(self.network.parameters(),\n",
        "                                            old_model.parameters()):\n",
        "      new_parameter.data.copy_((1 - self.tau)*new_parameter + \\\n",
        "                                      (self.tau)*old_parameter)\n",
        "                                    \n",
        "\n",
        "  def fit(self, elite_states, elite_actions):\n",
        "    for _ in range(self.n_batches):\n",
        "      self.optimizer.zero_grad()\n",
        "      states, actions = self.get_batch(elite_states, elite_actions)\n",
        "      predicted = self.network(states)\n",
        "      loss = self.loss(predicted, actions)\n",
        "      old_model = deepcopy(self.network)\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "      self.update_weights(old_model)\n",
        "      if self.noise_threshold > self.noise_threshold_min:\n",
        "        self.noise_threshold -= self.noise_threshold_decrease\n",
        "\n",
        "  def get_action(self, state):\n",
        "    state = torch.tensor(np.array([state]), dtype=torch.float)\n",
        "    action = self.network(state).detach().data.numpy()[0]\n",
        "    noise = self.noise_threshold * self.random_process.sample()\n",
        "    action = action + noise\n",
        "    return np.clip(action, - 2, + 2)\n",
        "\n",
        "class RandomProcess(object):\n",
        "    def reset_states(self):\n",
        "        pass\n",
        "\n",
        "class AnnealedGaussianProcess(RandomProcess):\n",
        "    def __init__(self, mu, sigma, sigma_min, n_steps_annealing):\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        self.n_steps = 0\n",
        "\n",
        "        if sigma_min is not None:\n",
        "            self.m = -float(sigma - sigma_min) / float(n_steps_annealing)\n",
        "            self.c = sigma\n",
        "            self.sigma_min = sigma_min\n",
        "        else:\n",
        "            self.m = 0.\n",
        "            self.c = sigma\n",
        "            self.sigma_min = sigma\n",
        "\n",
        "    @property\n",
        "    def current_sigma(self):\n",
        "        sigma = max(self.sigma_min, self.m * float(self.n_steps) + self.c)\n",
        "        return sigma\n",
        "\n",
        "class OrnsteinUhlenbeckProcess(AnnealedGaussianProcess):\n",
        "    def __init__(self, theta, mu=0., sigma=1., dt=1e-2, x0=None, size=1, sigma_min=None, n_steps_annealing=1000):\n",
        "        super(OrnsteinUhlenbeckProcess, self).__init__(mu=mu, sigma=sigma, sigma_min=sigma_min, n_steps_annealing=n_steps_annealing)\n",
        "        self.theta = theta\n",
        "        self.mu = mu\n",
        "        self.dt = dt\n",
        "        self.x0 = x0\n",
        "        self.size = size\n",
        "        self.reset_states()\n",
        "\n",
        "    def sample(self):\n",
        "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.current_sigma * np.sqrt(self.dt) * np.random.normal(size=self.size)\n",
        "        self.x_prev = x\n",
        "        self.n_steps += 1\n",
        "        return x\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.x_prev = self.x0 if self.x0 is not None else np.zeros(self.size)\n",
        "\n",
        "env = gym.make('Pendulum-v0')\n",
        "action_shape = env.action_space.shape\n",
        "space_shape = env.observation_space.shape\n",
        "agent = CEM_agent(space_shape, action_shape)\n",
        "\n",
        "n_epochs = 100\n",
        "n_sessions = 400\n",
        "percentile = 70\n",
        "for epoch in range(n_epochs):\n",
        "  generated_sessions = [generate_session(agent, env) for _ in range(n_sessions)]\n",
        "  states, actions, rewards = map(np.array, zip(*generated_sessions))\n",
        "  elite_states, elite_actions = select_elites(states, actions, rewards, percentile)\n",
        "  agent.fit(elite_states, elite_actions)\n",
        "  print(f'Epoch: {epoch}, mean reward: {np.mean(rewards)}')\n",
        "  show_progress()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, mean reward: -1308.807692035833\n",
            "Epoch: 1, mean reward: -1258.3305361469697\n",
            "Epoch: 2, mean reward: -1270.7012948255524\n",
            "Epoch: 3, mean reward: -1267.210231846825\n",
            "Epoch: 4, mean reward: -1251.6842664658864\n",
            "Epoch: 5, mean reward: -1247.588368669621\n",
            "Epoch: 6, mean reward: -1220.5119605020839\n",
            "Epoch: 7, mean reward: -1226.50562374322\n",
            "Epoch: 8, mean reward: -1221.4749116676792\n",
            "Epoch: 9, mean reward: -1222.4508127687423\n",
            "Epoch: 10, mean reward: -1224.878925007851\n",
            "Epoch: 11, mean reward: -1249.772585980991\n",
            "Epoch: 12, mean reward: -1239.6019319089546\n",
            "Epoch: 13, mean reward: -1271.5884652449872\n",
            "Epoch: 14, mean reward: -1212.9109154556636\n",
            "Epoch: 15, mean reward: -1234.3006045788827\n",
            "Epoch: 16, mean reward: -1222.5127462456912\n",
            "Epoch: 17, mean reward: -1241.4015490972859\n",
            "Epoch: 18, mean reward: -1210.940672721099\n",
            "Epoch: 19, mean reward: -1224.9003755325323\n",
            "Epoch: 20, mean reward: -1224.1365279530337\n",
            "Epoch: 21, mean reward: -1237.7062830298676\n",
            "Epoch: 22, mean reward: -1209.0056234603364\n",
            "Epoch: 23, mean reward: -1231.7407370745814\n",
            "Epoch: 24, mean reward: -1235.129699473453\n",
            "Epoch: 25, mean reward: -1242.7617818771528\n",
            "Epoch: 26, mean reward: -1192.3444185533992\n",
            "Epoch: 27, mean reward: -1200.8017928256295\n",
            "Epoch: 28, mean reward: -1237.2680978707958\n",
            "Epoch: 29, mean reward: -1206.9547357905158\n",
            "Epoch: 30, mean reward: -1217.1247819911325\n",
            "Epoch: 31, mean reward: -1220.682263734371\n",
            "Epoch: 32, mean reward: -1209.0728671270774\n",
            "Epoch: 33, mean reward: -1212.9916097650714\n",
            "Epoch: 34, mean reward: -1198.043110885113\n",
            "Epoch: 35, mean reward: -1222.0341500667512\n",
            "Epoch: 36, mean reward: -1202.6710290694887\n",
            "Epoch: 37, mean reward: -1227.5541651150925\n",
            "Epoch: 38, mean reward: -1250.3712350104022\n",
            "Epoch: 39, mean reward: -1200.150411487316\n",
            "Epoch: 40, mean reward: -1191.9023084575774\n",
            "Epoch: 41, mean reward: -1232.736125942737\n",
            "Epoch: 42, mean reward: -1204.1585112058717\n",
            "Epoch: 43, mean reward: -1221.6462129999275\n",
            "Epoch: 44, mean reward: -1221.8032656159928\n",
            "Epoch: 45, mean reward: -1213.0645310283003\n",
            "Epoch: 46, mean reward: -1219.361991492451\n",
            "Epoch: 47, mean reward: -1206.2904047135996\n",
            "Epoch: 48, mean reward: -1214.4161386288326\n",
            "Epoch: 49, mean reward: -1174.328559931505\n",
            "Epoch: 50, mean reward: -1198.7334584059133\n",
            "Epoch: 51, mean reward: -1209.470700320711\n",
            "Epoch: 52, mean reward: -1211.7480206658438\n",
            "Epoch: 53, mean reward: -1220.2373101375947\n",
            "Epoch: 54, mean reward: -1231.0050117527146\n",
            "Epoch: 55, mean reward: -1218.8445120371873\n",
            "Epoch: 56, mean reward: -1224.9201041551757\n",
            "Epoch: 57, mean reward: -1226.5168192972646\n",
            "Epoch: 58, mean reward: -1235.3799628507265\n",
            "Epoch: 59, mean reward: -1237.041871658337\n",
            "Epoch: 60, mean reward: -1210.9822767654075\n",
            "Epoch: 61, mean reward: -1195.8362950440076\n",
            "Epoch: 62, mean reward: -1244.5593135616186\n",
            "Epoch: 63, mean reward: -1192.451973761624\n",
            "Epoch: 64, mean reward: -1221.3617725714894\n",
            "Epoch: 65, mean reward: -1213.7881957159227\n",
            "Epoch: 66, mean reward: -1228.5101231504116\n",
            "Epoch: 67, mean reward: -1189.0280015902126\n",
            "Epoch: 68, mean reward: -1230.6496876530582\n",
            "Epoch: 69, mean reward: -1205.2850716954308\n",
            "Epoch: 70, mean reward: -1199.3943394267278\n",
            "Epoch: 71, mean reward: -1221.4403937764455\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-e23c99e9c9e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0mpercentile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m   \u001b[0mgenerated_sessions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgenerate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_sessions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m   \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgenerated_sessions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m   \u001b[0melite_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melite_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_elites\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercentile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-e23c99e9c9e8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0mpercentile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m   \u001b[0mgenerated_sessions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgenerate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_sessions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m   \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgenerated_sessions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m   \u001b[0melite_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melite_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_elites\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercentile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-f98d5e1d74d6>\u001b[0m in \u001b[0;36mgenerate_session\u001b[0;34m(agent, env, t_max)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mnew_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-e23c99e9c9e8>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise_threshold\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_state_dict_pre_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbPLf48R0skG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}