{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_Practice.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "4zeEHnnN3s5v"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfHyBLcU7uZd",
        "colab_type": "text"
      },
      "source": [
        "# Soliving Gym Pendulum-v0 task using CEM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOy0uVnXGth5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "%matplotlib inline\n",
        "from IPython.display import clear_output\n",
        "import gym.envs.toy_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqRcLmlHHXI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_session(alpha=0.1):\n",
        "  states, actions = [], []\n",
        "  total_reward = 0\n",
        "  s = env.reset()\n",
        "  while True:\n",
        "    action = agent.predict([s])\n",
        "    if random.random() < alpha:\n",
        "      action = [random.uniform(-2, 2)]\n",
        "    new_s, r, done, info = env.step(action)\n",
        "\n",
        "    states.append(s)\n",
        "    actions.append(action)\n",
        "    total_reward += r\n",
        "\n",
        "    s = new_s\n",
        "    if done:\n",
        "      break\n",
        "  return states, actions, total_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQrxdFDaH90H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def select_elites(states_batch, actions_batch, rewards_batch, percentile):\n",
        "    \"\"\"\n",
        "    Select states and actions from games that have rewards >= percentile\n",
        "    :param states_batch: list of lists of states, states_batch[session_i][t]\n",
        "    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n",
        "    :param rewards_batch: list of rewards, rewards_batch[session_i]\n",
        "\n",
        "    :returns: elite_states,elite_actions, both 1D lists of states and respective actions from elite sessions\n",
        "\n",
        "    Please return elite states and actions in their original order \n",
        "    [i.e. sorted by session number and timestep within session]\n",
        "\n",
        "    If you are confused, see examples below. Please don't assume that states are integers\n",
        "    (they will become different later).\n",
        "    \"\"\"\n",
        "\n",
        "    reward_threshold = np.percentile(rewards_batch, percentile)\n",
        "    elite_states, elite_actions = [], []\n",
        "    \n",
        "    for x in range(len(states_batch)):\n",
        "      if rewards_batch[x] >= reward_threshold:\n",
        "        elite_states.extend(states_batch[x])\n",
        "        elite_actions.extend(actions_batch[x])\n",
        "\n",
        "    return elite_states, elite_actions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEukmwR5IMR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_progress(rewards_batch, log, percentile, reward_range=[-990, +10]):\n",
        "    \"\"\"\n",
        "    A convenience function that displays training progress. \n",
        "    No cool math here, just charts.\n",
        "    \"\"\"\n",
        "\n",
        "    mean_reward = np.mean(rewards_batch)\n",
        "    threshold = np.percentile(rewards_batch, percentile)\n",
        "    log.append([mean_reward, threshold])\n",
        "\n",
        "    clear_output(True)\n",
        "    print(\"mean reward = %.3f, threshold=%.3f\" % (mean_reward, threshold))\n",
        "    plt.figure(figsize=[8, 4])\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(list(zip(*log))[0], label='Mean rewards')\n",
        "    plt.plot(list(zip(*log))[1], label='Reward thresholds')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.hist(rewards_batch, range=reward_range)\n",
        "    plt.vlines([np.percentile(rewards_batch, percentile)],\n",
        "               [0], [100], label=\"percentile\", color='red')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G638GEYMzGrq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"Pendulum-v0\")\n",
        "env.reset() # state - cos theta, sin theta, thetadot\n",
        "# Action and state space are continious, so we restrict our action space\n",
        "# by rounding actions with 0.001 precise\n",
        "min_torque, max_torque = env.action_space.low[0], env.action_space.high[0]\n",
        "actions = list(np.arange(min_torque, max_torque, 0.0001))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q63OMcyv8sIi",
        "colab_type": "text"
      },
      "source": [
        "## Solving Pendulum using MLPRegressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBFAeXTe8I81",
        "colab_type": "code",
        "outputId": "83c85717-664c-493a-9906-2e60ff9384b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Should play with parameters now\n",
        "agent = MLPRegressor(\n",
        "    hidden_layer_sizes=(50, 50),\n",
        "    activation='relu',\n",
        "    warm_start=True,\n",
        "    solver='adam',\n",
        "    learning_rate='constant'\n",
        ")\n",
        "agent.fit([env.reset()], [0.0001])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "             hidden_layer_sizes=(50, 50), learning_rate='constant',\n",
              "             learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
              "             n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
              "             random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
              "             validation_fraction=0.1, verbose=False, warm_start=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1pNBuKW8OcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_sessions = 100\n",
        "percentile = 70\n",
        "alpha = 0.1\n",
        "log = []\n",
        "for i in range(100):\n",
        "  sessions = [generate_session(alpha= alpha) for x in range(n_sessions)]\n",
        "  states_batch, actions_batch, rewards_batch = map(np.array, zip(*sessions))\n",
        "  elite_sessions, elite_actions = select_elites(states_batch, actions_batch, rewards_batch, percentile)\n",
        "  agent.fit(elite_sessions, elite_actions)\n",
        "  alpha*= 0.99\n",
        "  show_progress(rewards_batch, log, percentile, reward_range=[-2000, 500])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ4LjTdaEbPW",
        "colab_type": "text"
      },
      "source": [
        "# Solving Cart-Pole using Q-learning, Sarsa, EvSarsa\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySBwSPpr3fos",
        "colab_type": "text"
      },
      "source": [
        "## Q-learingn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUoX7T9XNzNJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, alpha, epsilon, discount, get_legal_actions):\n",
        "        \"\"\"\n",
        "        Q-Learning Agent\n",
        "        based on https://inst.eecs.berkeley.edu/~cs188/sp19/projects.html\n",
        "        Instance variables you have access to\n",
        "          - self.epsilon (exploration prob)\n",
        "          - self.alpha (learning rate)\n",
        "          - self.discount (discount rate aka gamma)\n",
        "\n",
        "        Functions you should use\n",
        "          - self.get_legal_actions(state) {state, hashable -> list of actions, each is hashable}\n",
        "            which returns legal actions for a state\n",
        "          - self.get_qvalue(state,action)\n",
        "            which returns Q(state,action)\n",
        "          - self.set_qvalue(state,action,value)\n",
        "            which sets Q(state,action) := value\n",
        "        !!!Important!!!\n",
        "        Note: please avoid using self._qValues directly. \n",
        "            There's a special self.get_qvalue/set_qvalue for that.\n",
        "        \"\"\"\n",
        "\n",
        "        self.get_legal_actions = get_legal_actions\n",
        "        self._qvalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "        self.alpha = alpha\n",
        "        self.epsilon = epsilon\n",
        "        self.discount = discount\n",
        "\n",
        "    def get_qvalue(self, state, action):\n",
        "        \"\"\" Returns Q(state,action) \"\"\"\n",
        "        return self._qvalues[state][action]\n",
        "\n",
        "    def set_qvalue(self, state, action, value):\n",
        "        \"\"\" Sets the Qvalue for [state,action] to the given value \"\"\"\n",
        "        self._qvalues[state][action] = value\n",
        "\n",
        "\n",
        "    def get_value(self, state):\n",
        "        \"\"\"\n",
        "        Compute your agent's estimate of V(s) using current q-values\n",
        "        V(s) = max_over_action Q(state,action) over possible actions.\n",
        "        Note: please take into account that q-values can be negative.\n",
        "        \"\"\"\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        # If there are no legal actions, return 0.0\n",
        "        if len(possible_actions) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        return max([self.get_qvalue(state, action) for action in possible_actions])\n",
        "\n",
        "    def update(self, state, action, reward, next_state):\n",
        "        \"\"\"\n",
        "        You should do your Q-Value update here:\n",
        "           Q(s,a) := (1 - alpha) * Q(s,a) + alpha * (r + gamma * V(s'))\n",
        "        \"\"\"\n",
        "\n",
        "        # agent parameters\n",
        "        gamma = self.discount\n",
        "        learning_rate = self.alpha\n",
        "\n",
        "        new_qvalue = (1 - learning_rate) * self.get_qvalue(state, action) + \\\n",
        "                      learning_rate * (reward + gamma * self.get_value(next_state))\n",
        "\n",
        "        self.set_qvalue(state, action, new_qvalue)\n",
        "\n",
        "    def get_best_action(self, state):\n",
        "        \"\"\"\n",
        "        Compute the best action to take in a state (using current q-values). \n",
        "        \"\"\"\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        # If there are no legal actions, return None\n",
        "        if len(possible_actions) == 0:\n",
        "            return None\n",
        "\n",
        "        best_action = max(possible_actions, \n",
        "                          key=lambda action: self.get_qvalue(state, action))\n",
        "\n",
        "        return best_action\n",
        "\n",
        "    def get_action(self, state):\n",
        "        \"\"\"\n",
        "        Compute the action to take in the current state, including exploration.  \n",
        "        With probability self.epsilon, we should take a random action.\n",
        "            otherwise - the best policy action (self.get_best_action).\n",
        "\n",
        "        Note: To pick randomly from a list, use random.choice(list). \n",
        "              To pick True or False with a given probablity, generate uniform number in [0, 1]\n",
        "              and compare it with your probability\n",
        "        \"\"\"\n",
        "\n",
        "        # Pick Action\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "        action = None\n",
        "\n",
        "        # If there are no legal actions, return None\n",
        "        if len(possible_actions) == 0:\n",
        "            return None\n",
        "\n",
        "        # agent parameters:\n",
        "        epsilon = self.epsilon\n",
        "\n",
        "        return self.get_best_action(state) if np.random.rand() > epsilon \\\n",
        "              else np.random.choice(possible_actions)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doDWD80jNzN8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gym.core import ObservationWrapper\n",
        "\n",
        "\n",
        "class Binarizer(ObservationWrapper):\n",
        "\n",
        "    def observation(self, state):\n",
        "\n",
        "        # state = <round state to some amount digits.>\n",
        "        # hint: you can do that with round(x,n_digits)\n",
        "        # you will need to pick a different n_digits for each dimension\n",
        "        state[0] = round(state[0])\n",
        "        state[1] = round(20*state[1])\n",
        "        state[2] = round(15*state[2])\n",
        "        state[3] = round(state[3])\n",
        "        return tuple(state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsADcRRAymdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_and_train(env, agent, t_max=10**4):\n",
        "    total_reward = 0.0\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        a = agent.get_action(s)\n",
        "        next_s, r, done, _ = env.step(a)\n",
        "        agent.update(s, a, r, next_s)\n",
        "\n",
        "        s = next_s\n",
        "        total_reward += r\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return total_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x266tArgNzNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = Binarizer(gym.make(\"CartPole-v0\"))\n",
        "n_actions = env.action_space.n\n",
        "agent = QLearningAgent(alpha=0.3, epsilon=0.25, discount=0.99,\n",
        "                        get_legal_actions=lambda s: range(n_actions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lq08qjeMvxiY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rewards = []\n",
        "for i in range(100000):\n",
        "    rewards.append(play_and_train(env, agent))\n",
        "    agent.epsilon *= 0.999\n",
        "    if i % 100 == 0:\n",
        "        clear_output(True)\n",
        "        print('eps =', agent.epsilon, 'mean reward =', np.mean(rewards[-10:]))\n",
        "        plt.plot(rewards)\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLsxc5Cy3rxh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zeEHnnN3s5v",
        "colab_type": "text"
      },
      "source": [
        "## Sarsa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTfgFZEV3xhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SarsaAgent(QLearningAgent):\n",
        "  def __init__(self, alpha, epsilon, discount, get_legal_actions):\n",
        "    super().__init__(alpha, epsilon, discount, get_legal_actions)\n",
        "\n",
        "  def update(self, state, action, reward, next_state, next_action):\n",
        "    gamma = self.discount\n",
        "    learning_rate = self.alpha\n",
        "    new_qvalue = (1 - learning_rate) * super().get_qvalue(next_state, next_action) + \\\n",
        "                      learning_rate * (reward + gamma * super().get_value(next_state))\n",
        "    super().set_qvalue(state, action, new_qvalue)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niOdENL3_0jr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_and_train(env, agent, t_max=10**4):\n",
        "    total_reward = 0.0\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        a = agent.get_action(s)\n",
        "        next_s, r, done, _ = env.step(a)\n",
        "        agent.update(s, a, r, next_s, agent.get_action(next_s))\n",
        "\n",
        "        s = next_s\n",
        "        total_reward += r\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return total_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhK3y1MrAmCJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = Binarizer(gym.make(\"CartPole-v0\"))\n",
        "n_actions = env.action_space.n\n",
        "agent = SarsaAgent(alpha=0.1, epsilon=0.25, discount=0.99,\n",
        "                        get_legal_actions=lambda s: range(n_actions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vx9-BRB2A22j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rewards = []\n",
        "for i in range(100000):\n",
        "    rewards.append(play_and_train(env, agent))\n",
        "    agent.epsilon *= 0.999\n",
        "    if i % 100 == 0:\n",
        "        clear_output(True)\n",
        "        print('eps =', agent.epsilon, 'mean reward =', np.mean(rewards[-10:]))\n",
        "        plt.plot(rewards)\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Frfu6swSzt3K",
        "colab_type": "text"
      },
      "source": [
        "##Expected Value SARSA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQHsJyd00AOn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EvSarsaAgent(SarsaAgent):\n",
        "  def get_value(self, state):\n",
        "        \"\"\" \n",
        "        Returns Vpi for current state under epsilon-greedy policy:\n",
        "          V_{pi}(s) = sum _{over a_i} {pi(a_i | s) * Q(s, a_i)}\n",
        "\n",
        "        Hint: all other methods from QLearningAgent are still accessible.\n",
        "        \"\"\"\n",
        "        epsilon = self.epsilon\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        # If there are no legal actions, return 0.0\n",
        "        if len(possible_actions) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        new_value = sum([(1/len(possible_actions))*self.get_qvalue(state, action) \\\n",
        "                       for action in possible_actions])\n",
        "        return state_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ldO4ZVq0UPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.envs.toy_text.CliffWalkingEnv()\n",
        "n_actions = env.action_space.n\n",
        "agent = EvSarsaAgent(alpha=0.3, epsilon=0.25, discount=0.99,\n",
        "                        get_legal_actions=lambda s: range(n_actions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kq50YMW0ccN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rewards = []\n",
        "for i in range(100000):\n",
        "    rewards.append(play_and_train(env, agent))\n",
        "    agent.epsilon *= 0.999\n",
        "    if i % 100 == 0:\n",
        "        clear_output(True)\n",
        "        print('eps =', agent.epsilon, 'mean reward =', np.mean(rewards[-10:]))\n",
        "        plt.plot(rewards)\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7332sLAJiS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}